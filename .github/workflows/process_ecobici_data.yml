name: Process and Save Dataset

on:
  workflow_dispatch:  # manual triggering
  schedule:
    - cron: '0 6 * * *'  # Runs daily at 6 UTC

jobs:
  process-data:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas pyarrow beautifulsoup4 requests pytz google-cloud-storage
    
    - name: Write GCP credentials decoded to file
      run: |
        echo "${{ secrets.GCP_SERVICE_ACCOUNT_JSON_BASE64 }}" | base64 -d > gcs-key.json
    
    - name: Set environment variable
      run: echo "GOOGLE_APPLICATION_CREDENTIALS=gcs-key.json" >> $GITHUB_ENV

    - name: Run data processing script
      run: python get_ecobici_data.py

    - name: Commit and push changes
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        git add partitioned_dataset/
        git commit -m "Update partitioned dataset"
        git push
      env:
        # Required if pushing back to the repo
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}